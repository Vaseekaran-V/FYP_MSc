{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import h5py\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchinfo import summary\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import time\n",
    "import copy\n",
    "from tqdm.auto import tqdm\n",
    "import torch.optim as optim\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_num = 13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with h5py.File('../../../data/3d_array/mod_test_data_3d_h5.h5', 'r') as f:\n",
    "    test_X = f['test_data_3d'][:]\n",
    "test_y = pd.read_parquet('../../../data/3d_array/test_targets.parquet')\n",
    "\n",
    "test_X = np.nan_to_num(test_X, nan=0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "end_of_month\n",
       "2018-03-31    137674\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_y['end_of_month'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vaseekaranv\\AppData\\Local\\Temp\\ipykernel_25024\\204564841.py:1: FutureWarning: The behavior of 'isin' with dtype=datetime64[ns] and castable values (e.g. strings) is deprecated. In a future version, these will not be considered matching by isin. Explicitly cast to the appropriate dtype before calling isin instead.\n",
      "  test_y = test_y[test_y['end_of_month'].isin(['2018-03-31'])]\n"
     ]
    }
   ],
   "source": [
    "test_y = test_y[test_y['end_of_month'].isin(['2018-03-31'])]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>customer_ID</th>\n",
       "      <th>end_of_month</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00000fd6641609c6ece5454664794f0340ad84dddce9a2...</td>\n",
       "      <td>2018-03-31</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000041bdba6ecadd89a52d11886e8eaaec9325906c9723...</td>\n",
       "      <td>2018-03-31</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>00007889e4fcd2614b6cbe7f8f3d2e5c728eca32d9eb8a...</td>\n",
       "      <td>2018-03-31</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0000d17a1447b25a01e42e1ac56b091bb7cbb06317be4c...</td>\n",
       "      <td>2018-03-31</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>00013181a0c5fc8f1ea38cd2b90fe8ad2fa8cad9d9f13e...</td>\n",
       "      <td>2018-03-31</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137669</th>\n",
       "      <td>fffee056e120fb326c9413fca5a7ab6618cc49be9bb6b1...</td>\n",
       "      <td>2018-03-31</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137670</th>\n",
       "      <td>fffee847c5c1af7dbdd36d98fea882893256c422cde86c...</td>\n",
       "      <td>2018-03-31</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137671</th>\n",
       "      <td>ffff41c8a52833b56430603969b9ca48d208e7c192c6a4...</td>\n",
       "      <td>2018-03-31</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137672</th>\n",
       "      <td>ffff9984b999fccb2b6127635ed0736dda94e544e67e02...</td>\n",
       "      <td>2018-03-31</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137673</th>\n",
       "      <td>ffffa5c46bc8de74f5a4554e74e239c8dee6b9baf38814...</td>\n",
       "      <td>2018-03-31</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>137674 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              customer_ID end_of_month  target\n",
       "0       00000fd6641609c6ece5454664794f0340ad84dddce9a2...   2018-03-31       0\n",
       "1       000041bdba6ecadd89a52d11886e8eaaec9325906c9723...   2018-03-31       0\n",
       "2       00007889e4fcd2614b6cbe7f8f3d2e5c728eca32d9eb8a...   2018-03-31       0\n",
       "3       0000d17a1447b25a01e42e1ac56b091bb7cbb06317be4c...   2018-03-31       0\n",
       "4       00013181a0c5fc8f1ea38cd2b90fe8ad2fa8cad9d9f13e...   2018-03-31       1\n",
       "...                                                   ...          ...     ...\n",
       "137669  fffee056e120fb326c9413fca5a7ab6618cc49be9bb6b1...   2018-03-31       0\n",
       "137670  fffee847c5c1af7dbdd36d98fea882893256c422cde86c...   2018-03-31       0\n",
       "137671  ffff41c8a52833b56430603969b9ca48d208e7c192c6a4...   2018-03-31       0\n",
       "137672  ffff9984b999fccb2b6127635ed0736dda94e544e67e02...   2018-03-31       0\n",
       "137673  ffffa5c46bc8de74f5a4554e74e239c8dee6b9baf38814...   2018-03-31       1\n",
       "\n",
       "[137674 rows x 3 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SEBlock(nn.Module):\n",
    "    def __init__(self, channel, reduction=16):\n",
    "        super(SEBlock, self).__init__()\n",
    "        self.avg_pool = nn.AdaptiveAvgPool1d(1)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(channel, channel // reduction, bias=False),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(channel // reduction, channel, bias=False),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, c, _ = x.size()\n",
    "        y = self.avg_pool(x).view(b, c)\n",
    "        y = self.fc(y).view(b, c, 1)\n",
    "        return x * y.expand_as(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MDC(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_sizes=[3, 5, 7], num_kernels_per_scale=3):\n",
    "        super(MDC, self).__init__()\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.kernel_sizes = kernel_sizes\n",
    "        self.num_scales = len(kernel_sizes)\n",
    "        self.num_kernels_per_scale = num_kernels_per_scale\n",
    "        self.total_kernels = self.num_scales * self.num_kernels_per_scale\n",
    "\n",
    "        # Attention mechanism to generate weights for kernels [cite: 142, 143, 145]\n",
    "        self.attention_pool = nn.AdaptiveAvgPool1d(1) # Pool across time dimension\n",
    "        # Conv1d to generate temporal attention map\n",
    "        self.attention_conv = nn.Conv1d(in_channels, self.total_kernels, kernel_size=1, bias=False)\n",
    "        self.attention_gap = nn.AdaptiveAvgPool1d(1) # Pool attention map to get weights\n",
    "        self.attention_activation = nn.Sigmoid()\n",
    "\n",
    "        # Store the kernels for each scale and each instance per scale\n",
    "        self.weights = nn.Parameter(torch.Tensor(self.total_kernels, out_channels, in_channels, 1)) # Use kernel_size=1 here, will expand later\n",
    "        nn.init.kaiming_uniform_(self.weights, a=np.sqrt(5)) # Initialize weights\n",
    "\n",
    "        # Store kernel sizes correctly\n",
    "        self.k_sizes = []\n",
    "        for k in kernel_sizes:\n",
    "            for _ in range(num_kernels_per_scale):\n",
    "                self.k_sizes.append(k)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, _, T = x.size()\n",
    "\n",
    "        # Calculate attention weights [cite: 142, 145]\n",
    "        pooled_x = self.attention_pool(x) # B x C_in x 1\n",
    "        # The paper's diagram (Fig 3) suggests GAP -> Conv -> GAP -> Sigmoid for attention\n",
    "        # Let's follow the diagram logic more closely:\n",
    "        # GAP on input: B x C_in x T -> B x C_in x 1\n",
    "        att_gap1 = self.attention_pool(x)\n",
    "        # Conv1d on pooled input: B x C_in x 1 -> B x total_kernels x 1 (This seems different from Fig 3's BxmxnxT output)\n",
    "        # Let's reinterpret Fig 3: GAP(x) [B,Cin,T]->[B,Cin,1], then Conv [B,Cin,1]->[B,mxn,1] (This doesn't match BxmxnxT)\n",
    "        # Let's try the alternative interpretation from the text (Section 3.2.1, Eq 2 implies GAP -> Conv -> GAP)\n",
    "        # GAP(x) -> B x Cin x 1\n",
    "        # wa * GAP(x) -> B x mxn x 1 (This still feels off from Fig 3 which applies conv before second GAP)\n",
    "\n",
    "        # Let's follow Fig 3 literally (ignoring the first GAP shown before Conv in the Attention block):\n",
    "        # Conv on input x: B x Cin x T -> B x mxn x T [cite: 143]\n",
    "        attention_map = self.attention_conv(x) # B x total_kernels x T\n",
    "        # GAP on attention_map: B x mxn x T -> B x mxn x 1 [cite: 145]\n",
    "        attention_vector = self.attention_gap(attention_map)\n",
    "        # Sigmoid activation: B x mxn x 1 [cite: 145]\n",
    "        attention_weights = self.attention_activation(attention_vector) # Shape: (batch_size, total_kernels, 1)\n",
    "\n",
    "        # Fuse kernels based on attention weights [cite: 140]\n",
    "        # Reshape weights for broadcasting: (batch_size, total_kernels, 1) -> (batch_size, total_kernels, 1, 1, 1)\n",
    "        # Reshape kernels: (total_kernels, out_c, in_c, 1)\n",
    "        # We need to perform convolution dynamically. PyTorch doesn't easily support dynamic kernel *shapes*.\n",
    "        # A common approach is to have fixed-size kernels and combine their *outputs*, or combine *weights* for a fixed kernel size.\n",
    "        # The paper seems to imply combining weights *before* convolution (Eq 1). This requires all kernels w_i^j to have the same size k.\n",
    "        # However, the goal is *multi-scale*.\n",
    "\n",
    "        # Let's implement the multi-scale aspect by having separate convolutions and combining outputs weighted by attention.\n",
    "        # This deviates slightly from Eq 1 but achieves the multi-scale goal.\n",
    "\n",
    "        outputs = []\n",
    "        current_kernel_idx = 0\n",
    "        for k_size in self.kernel_sizes:\n",
    "            padding = k_size // 2\n",
    "            for i in range(self.num_kernels_per_scale):\n",
    "                # Get the weights for this specific kernel instance (across all batches)\n",
    "                # attention_weights shape: B x total_kernels x 1\n",
    "                # kernel_weights shape: B x 1 x out_c x in_c x k_size\n",
    "                kernel_idx = current_kernel_idx + i\n",
    "                kernel_weight = self.weights[kernel_idx] # out_c x in_c x 1\n",
    "                # Manually create the kernel for Conv1d for this size\n",
    "                dynamic_kernel = kernel_weight.repeat(1, 1, k_size) # out_c x in_c x k_size\n",
    "                \n",
    "                # Perform convolution with this specific kernel\n",
    "                output_i = F.conv1d(x, dynamic_kernel, padding=padding) # B x out_c x T\n",
    "                \n",
    "                # Apply attention weight for this kernel\n",
    "                # attention_weights[:, kernel_idx, :] shape: B x 1 x 1\n",
    "                weighted_output = output_i * attention_weights[:, kernel_idx, :].unsqueeze(-1) # B x out_c x T\n",
    "                outputs.append(weighted_output)\n",
    "                \n",
    "            current_kernel_idx += self.num_kernels_per_scale\n",
    "\n",
    "        # Sum the weighted outputs from all kernels\n",
    "        # This is an alternative interpretation to fusing weights first.\n",
    "        out = torch.sum(torch.stack(outputs), dim=0) # B x out_c x T\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MDCResBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_sizes=[3, 5, 7], num_kernels_per_scale=3, stride=1):\n",
    "        super(MDCResBlock, self).__init__()\n",
    "        \n",
    "        # Use Conv1d for the residual connection if dimensions change\n",
    "        if stride != 1 or in_channels != out_channels:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv1d(in_channels, out_channels, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm1d(out_channels)\n",
    "            )\n",
    "        else:\n",
    "            self.shortcut = nn.Identity()\n",
    "\n",
    "        self.mdc1 = MDC(in_channels, out_channels, kernel_sizes, num_kernels_per_scale)\n",
    "        self.bn1 = nn.BatchNorm1d(out_channels)\n",
    "        # Paper uses PReLU, let's use ReLU for simplicity or add PReLU if needed. Using PReLU as per paper.\n",
    "        self.prelu1 = nn.PReLU(out_channels)\n",
    "\n",
    "        self.mdc2 = MDC(out_channels, out_channels, kernel_sizes, num_kernels_per_scale)\n",
    "        self.bn2 = nn.BatchNorm1d(out_channels)\n",
    "        self.prelu2 = nn.PReLU(out_channels) # Activation after final BN before adding shortcut\n",
    "\n",
    "    def forward(self, x):\n",
    "        shortcut = self.shortcut(x)\n",
    "\n",
    "        out = self.mdc1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.prelu1(out)\n",
    "\n",
    "        out = self.mdc2(out)\n",
    "        out = self.bn2(out)\n",
    "\n",
    "        out += shortcut\n",
    "        out = self.prelu2(out) # Final activation after adding shortcut\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MDCNet(nn.Module):\n",
    "    def __init__(self, input_channels, num_classes, block_channels=[128, 128, 128], kernel_sizes=[3,5,7], num_kernels_per_scale=2, dropout_rate=0.3, reduction=16):\n",
    "        super(MDCNet, self).__init__()\n",
    "        \n",
    "        self.blocks = nn.ModuleList()\n",
    "        current_channels = input_channels\n",
    "        \n",
    "        # Configuration from paper/Figure 2: 3 Blocks [cite: 130]\n",
    "        # Each block: MDC ResBlock -> MaxPool -> SE -> Dropout\n",
    "        for i, channels in enumerate(block_channels):\n",
    "            # Stride only applied if pooling happens, MaxPooling handles stride=2\n",
    "            res_block = MDCResBlock(current_channels, channels, kernel_sizes, num_kernels_per_scale, stride=1)\n",
    "            max_pool = nn.MaxPool1d(kernel_size=2, stride=2) # Halve the length [cite: 132]\n",
    "            se_block = SEBlock(channels, reduction) # SE block [cite: 133]\n",
    "            dropout = nn.Dropout(dropout_rate) # Dropout [cite: 134]\n",
    "            \n",
    "            block = nn.Sequential(\n",
    "                res_block,\n",
    "                max_pool,\n",
    "                se_block,\n",
    "                dropout\n",
    "            )\n",
    "            self.blocks.append(block)\n",
    "            current_channels = channels # Update channels for next block\n",
    "\n",
    "        self.gap = nn.AdaptiveAvgPool1d(1) # Global Average Pooling [cite: 130]\n",
    "        # The original ConvModel in experiment_10 has FC layers after pooling.\n",
    "        # MDCNet in the paper feeds features to DPNet or directly to Softmax.\n",
    "        # To make it a component usable in the existing structure, we output features before FC layers.\n",
    "        self.output_channels = current_channels\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Input x shape: B x T x C_in (e.g., B x 10 x 115)\n",
    "        # Conv1D expects B x C_in x T\n",
    "        x = x.permute(0, 2, 1) # B x C_in x T (e.g., B x 115 x 10)\n",
    "\n",
    "        for block in self.blocks:\n",
    "            x = block(x)\n",
    "\n",
    "        # Global Average Pooling\n",
    "        x = self.gap(x) # B x C_out x 1\n",
    "        x = x.view(x.size(0), -1) # Flatten: B x C_out\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModifiedConvModel(nn.Module):\n",
    "    def __init__(self, input_size, time_steps, num_classes=1, mdc_block_channels=[64, 64, 64], mdc_kernel_sizes=[3,5,7], mdc_kernels_per_scale=2, dropout_rate=0.2, fc_size=32):\n",
    "        super(ModifiedConvModel, self).__init__()\n",
    "        \n",
    "        # Instantiate MDCNet feature extractor\n",
    "        self.mdc_net = MDCNet(input_channels=input_size,\n",
    "                              num_classes=num_classes, # Not directly used by MDCNet output layer here\n",
    "                              block_channels=mdc_block_channels,\n",
    "                              kernel_sizes=mdc_kernel_sizes,\n",
    "                              num_kernels_per_scale=mdc_kernels_per_scale,\n",
    "                              dropout_rate=dropout_rate)\n",
    "        \n",
    "        # Get the output feature dimension from MDCNet\n",
    "        mdc_output_channels = self.mdc_net.output_channels\n",
    "\n",
    "        # Fully connected layers (similar to original experiment_10)\n",
    "        self.fc1 = nn.Linear(mdc_output_channels, fc_size)\n",
    "        self.relu_fc = nn.ReLU() # Added ReLU after FC1\n",
    "        self.fc2 = nn.Linear(fc_size, num_classes)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Input shape: batch_size x time_steps x features\n",
    "        \n",
    "        # Pass through MDCNet feature extractor\n",
    "        features = self.mdc_net(x) # Output: B x mdc_output_channels\n",
    "        \n",
    "        # Fully connected layers\n",
    "        x = self.fc1(features)\n",
    "        x = self.relu_fc(x)\n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        # Output probability\n",
    "        return self.sigmoid(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the Modified ConvModel\n",
    "input_size = test_X.shape[2]  # Number of features (115)\n",
    "time_steps = test_X.shape[1]  # Number of time steps (10)\n",
    "output_size = 1  # Binary classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model initialized with input_size=86, output_size=1\n"
     ]
    }
   ],
   "source": [
    "# --- MDCNet Hyperparameters ---\n",
    "# Using smaller channels than paper's example for potentially faster training/less memory\n",
    "mdc_block_channels = [16,32,64] # Example channel sizes for each block\n",
    "# Using kernel sizes mentioned in paper, and n=2 as found via cross-validation in paper [cite: 242, 243]\n",
    "mdc_kernel_sizes = [3, 5, 7]\n",
    "mdc_kernels_per_scale = 2\n",
    "dropout_rate = 0.2 # Adjusted dropout\n",
    "fc_size = 16 # Adjusted FC size\n",
    "\n",
    "# Create model instance\n",
    "model = ModifiedConvModel(input_size=input_size,\n",
    "                          time_steps=time_steps,\n",
    "                          num_classes=output_size,\n",
    "                          mdc_block_channels=mdc_block_channels,\n",
    "                          mdc_kernel_sizes=mdc_kernel_sizes,\n",
    "                          mdc_kernels_per_scale=mdc_kernels_per_scale,\n",
    "                          dropout_rate=dropout_rate,\n",
    "                          fc_size=fc_size)\n",
    "\n",
    "print(f\"Model initialized with input_size={input_size}, output_size={output_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vaseekaranv\\AppData\\Local\\Temp\\ipykernel_25024\\148194223.py:7: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(model_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error loading model parameters: Error(s) in loading state_dict for ModifiedConvModel:\n",
      "\tsize mismatch for mdc_net.blocks.0.0.shortcut.0.weight: copying a param with shape torch.Size([16, 115, 1]) from checkpoint, the shape in current model is torch.Size([16, 86, 1]).\n",
      "\tsize mismatch for mdc_net.blocks.0.0.mdc1.weights: copying a param with shape torch.Size([6, 16, 115, 1]) from checkpoint, the shape in current model is torch.Size([6, 16, 86, 1]).\n",
      "\tsize mismatch for mdc_net.blocks.0.0.mdc1.attention_conv.weight: copying a param with shape torch.Size([6, 115, 1]) from checkpoint, the shape in current model is torch.Size([6, 86, 1]).\n"
     ]
    }
   ],
   "source": [
    "# Define the model path\n",
    "model_path = f'../../../models/deep_learning/experiment_{experiment_num}.pth'\n",
    "\n",
    "# Load the model parameters\n",
    "try:\n",
    "    # Load the saved dictionary\n",
    "    checkpoint = torch.load(model_path)\n",
    "    \n",
    "    # Extract model parameters from the 'model_state_dict' key\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    print(f\"Model parameters loaded successfully from {model_path}\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"Model file not found at {model_path}\")\n",
    "    print(\"Please specify the correct path to the model parameters\")\n",
    "except KeyError:\n",
    "    print(f\"'model_state_dict' key not found in the checkpoint file\")\n",
    "    print(\"The file may have been saved with a different structure\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading model parameters: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "class TimeSeriesDataset(Dataset):\n",
    "    def __init__(self, data, targets):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            data: numpy array of shape (num_ids, time_steps, features)\n",
    "            targets: numpy array of shape (num_ids,)\n",
    "        \"\"\"\n",
    "        self.data = torch.FloatTensor(data)\n",
    "        self.targets = torch.FloatTensor(targets).unsqueeze(1)  # Add dimension for output\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx], self.targets[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 10000\n",
    "test_dataset = TimeSeriesDataset(test_X, test_y['target'].values)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9153    0.0022    0.0044    102026\n",
      "           1     0.2592    0.9994    0.4117     35648\n",
      "\n",
      "    accuracy                         0.2604    137674\n",
      "   macro avg     0.5873    0.5008    0.2081    137674\n",
      "weighted avg     0.7454    0.2604    0.1099    137674\n",
      "\n",
      "Accuracy: 0.2604\n",
      "ROC-AUC Score: 0.6170\n",
      "\n",
      "Confusion Matrix:\n",
      "[[   227 101799]\n",
      " [    21  35627]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, accuracy_score, roc_auc_score, confusion_matrix\n",
    "\n",
    "# Set model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Check if CUDA is available and move model to the appropriate device\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "model = model.to(device)\n",
    "\n",
    "# Lists to store predictions and true values\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "\n",
    "# Perform inference without gradient calculation\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_loader:\n",
    "        # Move inputs and labels to the appropriate device\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "        \n",
    "        # Store predictions and labels\n",
    "        all_preds.append(outputs.cpu().numpy())\n",
    "        all_labels.append(labels.cpu().numpy())\n",
    "\n",
    "# Concatenate all batches\n",
    "all_preds = np.concatenate(all_preds)\n",
    "all_labels = np.concatenate(all_labels)\n",
    "\n",
    "# Convert predictions to binary (0 or 1) using threshold of 0.5\n",
    "pred_classes = (all_preds > 0.5).astype(int)\n",
    "true_classes = all_labels.astype(int)\n",
    "\n",
    "# Generate classification report\n",
    "\n",
    "# Print classification report\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(true_classes, pred_classes, digits = 4))\n",
    "\n",
    "# Calculate and print accuracy\n",
    "accuracy = accuracy_score(true_classes, pred_classes)\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# Calculate and print ROC-AUC score\n",
    "auc = roc_auc_score(true_classes, all_preds)\n",
    "print(f\"ROC-AUC Score: {auc:.4f}\")\n",
    "\n",
    "# Print confusion matrix\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(confusion_matrix(true_classes, pred_classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nibm_dl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
