{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import h5py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_num = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with h5py.File('../../../data/3d_array/test_data_3d_h5.h5', 'r') as f:\n",
    "    test_X = f['test_data_3d'][:]\n",
    "test_y = pd.read_parquet('../../../data/3d_array/test_targets.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "end_of_month\n",
       "2017-06-30    405607\n",
       "2017-07-31    405607\n",
       "2017-08-31    405607\n",
       "2017-09-30    405607\n",
       "2017-10-31    405607\n",
       "2017-11-30    405607\n",
       "2017-12-31    405607\n",
       "2018-01-31    405607\n",
       "2018-02-28    405607\n",
       "2018-03-31    405607\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_y['end_of_month'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vaseekaranv\\AppData\\Local\\Temp\\ipykernel_37732\\204564841.py:1: FutureWarning: The behavior of 'isin' with dtype=datetime64[ns] and castable values (e.g. strings) is deprecated. In a future version, these will not be considered matching by isin. Explicitly cast to the appropriate dtype before calling isin instead.\n",
      "  test_y = test_y[test_y['end_of_month'].isin(['2018-03-31'])]\n"
     ]
    }
   ],
   "source": [
    "test_y = test_y[test_y['end_of_month'].isin(['2018-03-31'])]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>customer_ID</th>\n",
       "      <th>end_of_month</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0000099d6bd597052cdcda90ffabf56573fe9d7c79be5f...</td>\n",
       "      <td>2018-03-31</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>00000fd6641609c6ece5454664794f0340ad84dddce9a2...</td>\n",
       "      <td>2018-03-31</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>00001b22f846c82c51f6e3958ccd81970162bae8b007e8...</td>\n",
       "      <td>2018-03-31</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>000041bdba6ecadd89a52d11886e8eaaec9325906c9723...</td>\n",
       "      <td>2018-03-31</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>00007889e4fcd2614b6cbe7f8f3d2e5c728eca32d9eb8a...</td>\n",
       "      <td>2018-03-31</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4056029</th>\n",
       "      <td>ffff41c8a52833b56430603969b9ca48d208e7c192c6a4...</td>\n",
       "      <td>2018-03-31</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4056039</th>\n",
       "      <td>ffff518bb2075e4816ee3fe9f3b152c57fc0e6f01bf7fd...</td>\n",
       "      <td>2018-03-31</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4056049</th>\n",
       "      <td>ffff9984b999fccb2b6127635ed0736dda94e544e67e02...</td>\n",
       "      <td>2018-03-31</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4056059</th>\n",
       "      <td>ffffa5c46bc8de74f5a4554e74e239c8dee6b9baf38814...</td>\n",
       "      <td>2018-03-31</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4056069</th>\n",
       "      <td>fffff1d38b785cef84adeace64f8f83db3a0c31e8d92ea...</td>\n",
       "      <td>2018-03-31</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>405607 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               customer_ID end_of_month  \\\n",
       "9        0000099d6bd597052cdcda90ffabf56573fe9d7c79be5f...   2018-03-31   \n",
       "19       00000fd6641609c6ece5454664794f0340ad84dddce9a2...   2018-03-31   \n",
       "29       00001b22f846c82c51f6e3958ccd81970162bae8b007e8...   2018-03-31   \n",
       "39       000041bdba6ecadd89a52d11886e8eaaec9325906c9723...   2018-03-31   \n",
       "49       00007889e4fcd2614b6cbe7f8f3d2e5c728eca32d9eb8a...   2018-03-31   \n",
       "...                                                    ...          ...   \n",
       "4056029  ffff41c8a52833b56430603969b9ca48d208e7c192c6a4...   2018-03-31   \n",
       "4056039  ffff518bb2075e4816ee3fe9f3b152c57fc0e6f01bf7fd...   2018-03-31   \n",
       "4056049  ffff9984b999fccb2b6127635ed0736dda94e544e67e02...   2018-03-31   \n",
       "4056059  ffffa5c46bc8de74f5a4554e74e239c8dee6b9baf38814...   2018-03-31   \n",
       "4056069  fffff1d38b785cef84adeace64f8f83db3a0c31e8d92ea...   2018-03-31   \n",
       "\n",
       "         target  \n",
       "9             0  \n",
       "19            0  \n",
       "29            0  \n",
       "39            0  \n",
       "49            0  \n",
       "...         ...  \n",
       "4056029       0  \n",
       "4056039       0  \n",
       "4056049       0  \n",
       "4056059       1  \n",
       "4056069       0  \n",
       "\n",
       "[405607 rows x 3 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "import torch\n",
    "\n",
    "class ParallelConvBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_sizes=[3, 5, 7], dropout_rate=0.2):\n",
    "        \"\"\"\n",
    "        Parallel Convolutional Block that processes input through multiple convolutional paths\n",
    "        with different kernel sizes and concatenates the results.\n",
    "        \n",
    "        Args:\n",
    "            in_channels: Number of input channels\n",
    "            out_channels: Number of output channels\n",
    "            kernel_sizes: List of kernel sizes for parallel convolutions\n",
    "            dropout_rate: Dropout probability\n",
    "        \"\"\"\n",
    "        super(ParallelConvBlock, self).__init__()\n",
    "        \n",
    "        self.n_paths = len(kernel_sizes)\n",
    "        # Calculate channels per path\n",
    "        path_channels = out_channels // self.n_paths\n",
    "        \n",
    "        # Create parallel convolutional paths\n",
    "        self.paths = nn.ModuleList()\n",
    "        for k_size in kernel_sizes:\n",
    "            padding = k_size // 2  # Same padding to maintain sequence length\n",
    "            path = nn.Sequential(\n",
    "                nn.Conv1d(in_channels, path_channels, kernel_size=k_size, padding=padding),\n",
    "                nn.BatchNorm1d(path_channels),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(dropout_rate)\n",
    "            )\n",
    "            self.paths.append(path)\n",
    "            \n",
    "        # Projection layer to ensure output has exactly out_channels\n",
    "        self.projection = nn.Sequential(\n",
    "            nn.Conv1d(path_channels * self.n_paths, out_channels, kernel_size=1),\n",
    "            nn.BatchNorm1d(out_channels),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Process input through parallel paths\n",
    "        outputs = [path(x) for path in self.paths]\n",
    "        \n",
    "        # Concatenate along channel dimension\n",
    "        # print(f\"Outputs shape before concatenation: {[out.shape for out in outputs]}\")\n",
    "        x = torch.cat(outputs, dim=1)\n",
    "        \n",
    "        # Apply projection to get final output\n",
    "        x = self.projection(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=3, padding=1, pool=True, dropout=0.3):\n",
    "        super(ConvBlock, self).__init__()\n",
    "        self.conv = nn.Conv1d(in_channels, out_channels, kernel_size, padding=padding)\n",
    "        self.bn = nn.BatchNorm1d(out_channels)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.pool = nn.MaxPool1d(kernel_size=2) if pool else None\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = self.bn(x)\n",
    "        x = self.relu(x)\n",
    "        if self.pool:\n",
    "            x = self.pool(x)\n",
    "        x = self.dropout(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvModel(nn.Module):\n",
    "    def __init__(self, input_size, output_size=1):\n",
    "        super(ConvModel, self).__init__()\n",
    "        \n",
    "        # First convolutional block\n",
    "        self.parallel_conv = ParallelConvBlock(input_size, 100, kernel_sizes=[3, 5, 7, 9], dropout_rate=0.2)\n",
    "        \n",
    "        # Second convolutional block\n",
    "        self.block2 = ConvBlock(100, 64, dropout=0.2)\n",
    "        \n",
    "        # Third convolutional block\n",
    "        self.block3 = ConvBlock(64, 32, pool=False, dropout=0.2)  # No pooling in the last block\n",
    "        \n",
    "        # Global pooling\n",
    "        self.global_avg_pool = nn.AdaptiveAvgPool1d(1)\n",
    "        \n",
    "        # Fully connected layers\n",
    "        self.fc1 = nn.Linear(32, 16)\n",
    "        self.fc2 = nn.Linear(16, output_size)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Input shape: batch_size x time_steps x features\n",
    "        # For Conv1D: convert to batch_size x features(channels) x time_steps\n",
    "        x = x.permute(0, 2, 1)\n",
    "        \n",
    "        # Pass through convolutional blocks\n",
    "        x = self.parallel_conv(x)\n",
    "        x = self.block2(x)\n",
    "        x = self.block3(x)\n",
    "        \n",
    "        # Global pooling to get fixed-size representation\n",
    "        x = self.global_avg_pool(x)\n",
    "        x = x.view(x.size(0), -1)  # Flatten\n",
    "        \n",
    "        # Fully connected layers\n",
    "        x = self.fc1(x)\n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        # Output probability\n",
    "        return self.sigmoid(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model initialized with input_size=115, output_size=1\n"
     ]
    }
   ],
   "source": [
    "# Initialize the ConvModel\n",
    "input_size = test_X.shape[2]  # Number of features\n",
    "output_size = 1  # Binary classification\n",
    "\n",
    "# Create model instance\n",
    "model = ConvModel(input_size=input_size, output_size=output_size)\n",
    "print(f\"Model initialized with input_size={input_size}, output_size={output_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model parameters loaded successfully from ../../../models/deep_learning/experiment_10.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vaseekaranv\\AppData\\Local\\Temp\\ipykernel_37732\\148194223.py:7: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(model_path)\n"
     ]
    }
   ],
   "source": [
    "# Define the model path\n",
    "model_path = f'../../../models/deep_learning/experiment_{experiment_num}.pth'\n",
    "\n",
    "# Load the model parameters\n",
    "try:\n",
    "    # Load the saved dictionary\n",
    "    checkpoint = torch.load(model_path)\n",
    "    \n",
    "    # Extract model parameters from the 'model_state_dict' key\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    print(f\"Model parameters loaded successfully from {model_path}\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"Model file not found at {model_path}\")\n",
    "    print(\"Please specify the correct path to the model parameters\")\n",
    "except KeyError:\n",
    "    print(f\"'model_state_dict' key not found in the checkpoint file\")\n",
    "    print(\"The file may have been saved with a different structure\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading model parameters: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "class TimeSeriesDataset(Dataset):\n",
    "    def __init__(self, data, targets):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            data: numpy array of shape (num_ids, time_steps, features)\n",
    "            targets: numpy array of shape (num_ids,)\n",
    "        \"\"\"\n",
    "        self.data = torch.FloatTensor(data)\n",
    "        self.targets = torch.FloatTensor(targets).unsqueeze(1)  # Add dimension for output\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx], self.targets[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 10000\n",
    "test_dataset = TimeSeriesDataset(test_X, test_y['target'].values)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9431    0.9255    0.9342    307381\n",
      "           1     0.7797    0.8251    0.8017     98226\n",
      "\n",
      "    accuracy                         0.9012    405607\n",
      "   macro avg     0.8614    0.8753    0.8680    405607\n",
      "weighted avg     0.9035    0.9012    0.9021    405607\n",
      "\n",
      "Accuracy: 0.9012\n",
      "ROC-AUC Score: 0.9577\n",
      "\n",
      "Confusion Matrix:\n",
      "[[284478  22903]\n",
      " [ 17179  81047]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, accuracy_score, roc_auc_score, confusion_matrix\n",
    "\n",
    "# Set model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Check if CUDA is available and move model to the appropriate device\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "model = model.to(device)\n",
    "\n",
    "# Lists to store predictions and true values\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "\n",
    "# Perform inference without gradient calculation\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_loader:\n",
    "        # Move inputs and labels to the appropriate device\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "        \n",
    "        # Store predictions and labels\n",
    "        all_preds.append(outputs.cpu().numpy())\n",
    "        all_labels.append(labels.cpu().numpy())\n",
    "\n",
    "# Concatenate all batches\n",
    "all_preds = np.concatenate(all_preds)\n",
    "all_labels = np.concatenate(all_labels)\n",
    "\n",
    "# Convert predictions to binary (0 or 1) using threshold of 0.5\n",
    "pred_classes = (all_preds > 0.5).astype(int)\n",
    "true_classes = all_labels.astype(int)\n",
    "\n",
    "# Generate classification report\n",
    "\n",
    "# Print classification report\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(true_classes, pred_classes, digits = 4))\n",
    "\n",
    "# Calculate and print accuracy\n",
    "accuracy = accuracy_score(true_classes, pred_classes)\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# Calculate and print ROC-AUC score\n",
    "auc = roc_auc_score(true_classes, all_preds)\n",
    "print(f\"ROC-AUC Score: {auc:.4f}\")\n",
    "\n",
    "# Print confusion matrix\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(confusion_matrix(true_classes, pred_classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nibm_dl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
