{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import h5py\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchinfo import summary\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import time\n",
    "import copy\n",
    "from tqdm.auto import tqdm\n",
    "import torch.optim as optim\n",
    "import os\n",
    "import sys\n",
    "\n",
    "import random\n",
    "\n",
    "#trying to ensure reproducibility\n",
    "torch.manual_seed(0)\n",
    "random.seed(0)\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting path to load util functions\n",
    "from pathlib import Path\n",
    "parent_dir = Path.cwd().parents[1]\n",
    "sys.path.append(os.path.abspath(parent_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_num = 14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading data\n",
    "with h5py.File('../../data/3d_array/mod_train_data_3d_h5.h5', 'r') as f:\n",
    "    train_X = f['train_data_3d'][:]\n",
    "with h5py.File('../../data/3d_array/mod_val_data_3d_h5.h5', 'r') as f:\n",
    "    val_X = f['val_data_3d'][:]\n",
    "# with h5py.File('../../data/3d_array/test_data_3d_h5.h5', 'r') as f:\n",
    "#     test_X = f['test_data_3d'][:]\n",
    "\n",
    "train_y = pd.read_parquet('../../data/3d_array/train_targets.parquet')\n",
    "val_y = pd.read_parquet('../../data/3d_array/val_targets.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X = np.nan_to_num(train_X, nan = 0.0)\n",
    "val_X = np.nan_to_num(val_X, nan = 0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "end_of_month\n",
       "2018-03-31    289115\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_y['end_of_month'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vaseekaranv\\AppData\\Local\\Temp\\ipykernel_8724\\639591509.py:1: FutureWarning: The behavior of 'isin' with dtype=datetime64[ns] and castable values (e.g. strings) is deprecated. In a future version, these will not be considered matching by isin. Explicitly cast to the appropriate dtype before calling isin instead.\n",
      "  train_y = train_y[train_y['end_of_month'].isin(['2018-03-31'])]\n",
      "C:\\Users\\vaseekaranv\\AppData\\Local\\Temp\\ipykernel_8724\\639591509.py:2: FutureWarning: The behavior of 'isin' with dtype=datetime64[ns] and castable values (e.g. strings) is deprecated. In a future version, these will not be considered matching by isin. Explicitly cast to the appropriate dtype before calling isin instead.\n",
      "  val_y = val_y[val_y['end_of_month'].isin(['2018-03-31'])]\n"
     ]
    }
   ],
   "source": [
    "train_y = train_y[train_y['end_of_month'].isin(['2018-03-31'])]\n",
    "val_y = val_y[val_y['end_of_month'].isin(['2018-03-31'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>customer_ID</th>\n",
       "      <th>end_of_month</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0000099d6bd597052cdcda90ffabf56573fe9d7c79be5f...</td>\n",
       "      <td>2018-03-31</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>00001b22f846c82c51f6e3958ccd81970162bae8b007e8...</td>\n",
       "      <td>2018-03-31</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>000084e5023181993c2e1b665ac88dbb1ce9ef621ec537...</td>\n",
       "      <td>2018-03-31</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>000098081fde4fd64bc4d503a5d6f86a0aedc425c96f52...</td>\n",
       "      <td>2018-03-31</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0000f99513770170a1aba690daeeb8a96da4a39f11fc27...</td>\n",
       "      <td>2018-03-31</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>289110</th>\n",
       "      <td>fffe3ec7cdbc1caac845c884b389ed347bfc1da9d09731...</td>\n",
       "      <td>2018-03-31</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>289111</th>\n",
       "      <td>fffef3305f19a11fb6c15f4ebe9be1bd664540e57c0a6a...</td>\n",
       "      <td>2018-03-31</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>289112</th>\n",
       "      <td>ffff39cc22a375d07369980d02d617883dd28ad81a6aa3...</td>\n",
       "      <td>2018-03-31</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>289113</th>\n",
       "      <td>ffff518bb2075e4816ee3fe9f3b152c57fc0e6f01bf7fd...</td>\n",
       "      <td>2018-03-31</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>289114</th>\n",
       "      <td>fffff1d38b785cef84adeace64f8f83db3a0c31e8d92ea...</td>\n",
       "      <td>2018-03-31</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>289115 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              customer_ID end_of_month  target\n",
       "0       0000099d6bd597052cdcda90ffabf56573fe9d7c79be5f...   2018-03-31       0\n",
       "1       00001b22f846c82c51f6e3958ccd81970162bae8b007e8...   2018-03-31       0\n",
       "2       000084e5023181993c2e1b665ac88dbb1ce9ef621ec537...   2018-03-31       0\n",
       "3       000098081fde4fd64bc4d503a5d6f86a0aedc425c96f52...   2018-03-31       0\n",
       "4       0000f99513770170a1aba690daeeb8a96da4a39f11fc27...   2018-03-31       1\n",
       "...                                                   ...          ...     ...\n",
       "289110  fffe3ec7cdbc1caac845c884b389ed347bfc1da9d09731...   2018-03-31       1\n",
       "289111  fffef3305f19a11fb6c15f4ebe9be1bd664540e57c0a6a...   2018-03-31       0\n",
       "289112  ffff39cc22a375d07369980d02d617883dd28ad81a6aa3...   2018-03-31       0\n",
       "289113  ffff518bb2075e4816ee3fe9f3b152c57fc0e6f01bf7fd...   2018-03-31       0\n",
       "289114  fffff1d38b785cef84adeace64f8f83db3a0c31e8d92ea...   2018-03-31       0\n",
       "\n",
       "[289115 rows x 3 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_y.sort_values(by=['customer_ID'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((289115, 13, 86), (289115, 3))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_X.shape, train_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((32124, 13, 86), (32124, 3))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_X.shape, val_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SEBlock(nn.Module):\n",
    "    def __init__(self, channel, reduction=16):\n",
    "        super(SEBlock, self).__init__()\n",
    "        self.avg_pool = nn.AdaptiveAvgPool1d(1)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(channel, channel // reduction, bias=False),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(channel // reduction, channel, bias=False),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, c, _ = x.size()\n",
    "        y = self.avg_pool(x).view(b, c)\n",
    "        y = self.fc(y).view(b, c, 1)\n",
    "        return x * y.expand_as(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MDC(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_sizes=[3, 5, 7], num_kernels_per_scale=3):\n",
    "        super(MDC, self).__init__()\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.kernel_sizes = kernel_sizes\n",
    "        self.num_scales = len(kernel_sizes)\n",
    "        self.num_kernels_per_scale = num_kernels_per_scale\n",
    "        self.total_kernels = self.num_scales * self.num_kernels_per_scale\n",
    "\n",
    "        # Attention mechanism to generate weights for kernels [cite: 142, 143, 145]\n",
    "        self.attention_pool = nn.AdaptiveAvgPool1d(1) # Pool across time dimension\n",
    "        # Conv1d to generate temporal attention map\n",
    "        self.attention_conv = nn.Conv1d(in_channels, self.total_kernels, kernel_size=1, bias=False)\n",
    "        self.attention_gap = nn.AdaptiveAvgPool1d(1) # Pool attention map to get weights\n",
    "        self.attention_activation = nn.Sigmoid()\n",
    "\n",
    "        # Store the kernels for each scale and each instance per scale\n",
    "        self.weights = nn.Parameter(torch.Tensor(self.total_kernels, out_channels, in_channels, 1)) # Use kernel_size=1 here, will expand later\n",
    "        nn.init.kaiming_uniform_(self.weights, a=np.sqrt(5)) # Initialize weights\n",
    "\n",
    "        # Store kernel sizes correctly\n",
    "        self.k_sizes = []\n",
    "        for k in kernel_sizes:\n",
    "            for _ in range(num_kernels_per_scale):\n",
    "                self.k_sizes.append(k)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, _, T = x.size()\n",
    "\n",
    "        # Calculate attention weights [cite: 142, 145]\n",
    "        pooled_x = self.attention_pool(x) # B x C_in x 1\n",
    "        # The paper's diagram (Fig 3) suggests GAP -> Conv -> GAP -> Sigmoid for attention\n",
    "        # Let's follow the diagram logic more closely:\n",
    "        # GAP on input: B x C_in x T -> B x C_in x 1\n",
    "        att_gap1 = self.attention_pool(x)\n",
    "        # Conv1d on pooled input: B x C_in x 1 -> B x total_kernels x 1 (This seems different from Fig 3's BxmxnxT output)\n",
    "        # Let's reinterpret Fig 3: GAP(x) [B,Cin,T]->[B,Cin,1], then Conv [B,Cin,1]->[B,mxn,1] (This doesn't match BxmxnxT)\n",
    "        # Let's try the alternative interpretation from the text (Section 3.2.1, Eq 2 implies GAP -> Conv -> GAP)\n",
    "        # GAP(x) -> B x Cin x 1\n",
    "        # wa * GAP(x) -> B x mxn x 1 (This still feels off from Fig 3 which applies conv before second GAP)\n",
    "\n",
    "        # Let's follow Fig 3 literally (ignoring the first GAP shown before Conv in the Attention block):\n",
    "        # Conv on input x: B x Cin x T -> B x mxn x T [cite: 143]\n",
    "        attention_map = self.attention_conv(x) # B x total_kernels x T\n",
    "        # GAP on attention_map: B x mxn x T -> B x mxn x 1 [cite: 145]\n",
    "        attention_vector = self.attention_gap(attention_map)\n",
    "        # Sigmoid activation: B x mxn x 1 [cite: 145]\n",
    "        attention_weights = self.attention_activation(attention_vector) # Shape: (batch_size, total_kernels, 1)\n",
    "\n",
    "        # Fuse kernels based on attention weights [cite: 140]\n",
    "        # Reshape weights for broadcasting: (batch_size, total_kernels, 1) -> (batch_size, total_kernels, 1, 1, 1)\n",
    "        # Reshape kernels: (total_kernels, out_c, in_c, 1)\n",
    "        # We need to perform convolution dynamically. PyTorch doesn't easily support dynamic kernel *shapes*.\n",
    "        # A common approach is to have fixed-size kernels and combine their *outputs*, or combine *weights* for a fixed kernel size.\n",
    "        # The paper seems to imply combining weights *before* convolution (Eq 1). This requires all kernels w_i^j to have the same size k.\n",
    "        # However, the goal is *multi-scale*.\n",
    "\n",
    "        # Let's implement the multi-scale aspect by having separate convolutions and combining outputs weighted by attention.\n",
    "        # This deviates slightly from Eq 1 but achieves the multi-scale goal.\n",
    "\n",
    "        outputs = []\n",
    "        current_kernel_idx = 0\n",
    "        for k_size in self.kernel_sizes:\n",
    "            padding = k_size // 2\n",
    "            for i in range(self.num_kernels_per_scale):\n",
    "                # Get the weights for this specific kernel instance (across all batches)\n",
    "                # attention_weights shape: B x total_kernels x 1\n",
    "                # kernel_weights shape: B x 1 x out_c x in_c x k_size\n",
    "                kernel_idx = current_kernel_idx + i\n",
    "                kernel_weight = self.weights[kernel_idx] # out_c x in_c x 1\n",
    "                # Manually create the kernel for Conv1d for this size\n",
    "                dynamic_kernel = kernel_weight.repeat(1, 1, k_size) # out_c x in_c x k_size\n",
    "                \n",
    "                # Perform convolution with this specific kernel\n",
    "                output_i = F.conv1d(x, dynamic_kernel, padding=padding) # B x out_c x T\n",
    "                \n",
    "                # Apply attention weight for this kernel\n",
    "                # attention_weights[:, kernel_idx, :] shape: B x 1 x 1\n",
    "                weighted_output = output_i * attention_weights[:, kernel_idx, :].unsqueeze(-1) # B x out_c x T\n",
    "                outputs.append(weighted_output)\n",
    "                \n",
    "            current_kernel_idx += self.num_kernels_per_scale\n",
    "\n",
    "        # Sum the weighted outputs from all kernels\n",
    "        # This is an alternative interpretation to fusing weights first.\n",
    "        out = torch.sum(torch.stack(outputs), dim=0) # B x out_c x T\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MDCResBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_sizes=[3, 5, 7], num_kernels_per_scale=3, stride=1):\n",
    "        super(MDCResBlock, self).__init__()\n",
    "        \n",
    "        # Use Conv1d for the residual connection if dimensions change\n",
    "        if stride != 1 or in_channels != out_channels:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv1d(in_channels, out_channels, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm1d(out_channels)\n",
    "            )\n",
    "        else:\n",
    "            self.shortcut = nn.Identity()\n",
    "\n",
    "        self.mdc1 = MDC(in_channels, out_channels, kernel_sizes, num_kernels_per_scale)\n",
    "        self.bn1 = nn.BatchNorm1d(out_channels)\n",
    "        # Paper uses PReLU, let's use ReLU for simplicity or add PReLU if needed. Using PReLU as per paper.\n",
    "        self.prelu1 = nn.PReLU(out_channels)\n",
    "\n",
    "        self.mdc2 = MDC(out_channels, out_channels, kernel_sizes, num_kernels_per_scale)\n",
    "        self.bn2 = nn.BatchNorm1d(out_channels)\n",
    "        self.prelu2 = nn.PReLU(out_channels) # Activation after final BN before adding shortcut\n",
    "\n",
    "    def forward(self, x):\n",
    "        shortcut = self.shortcut(x)\n",
    "\n",
    "        out = self.mdc1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.prelu1(out)\n",
    "\n",
    "        out = self.mdc2(out)\n",
    "        out = self.bn2(out)\n",
    "\n",
    "        out += shortcut\n",
    "        out = self.prelu2(out) # Final activation after adding shortcut\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MDCNet(nn.Module):\n",
    "    def __init__(self, input_channels, num_classes, block_channels=[128, 128, 128], kernel_sizes=[3,5,7], num_kernels_per_scale=2, dropout_rate=0.3, reduction=16):\n",
    "        super(MDCNet, self).__init__()\n",
    "        \n",
    "        self.blocks = nn.ModuleList()\n",
    "        current_channels = input_channels\n",
    "        \n",
    "        # Configuration from paper/Figure 2: 3 Blocks [cite: 130]\n",
    "        # Each block: MDC ResBlock -> MaxPool -> SE -> Dropout\n",
    "        for i, channels in enumerate(block_channels):\n",
    "            # Stride only applied if pooling happens, MaxPooling handles stride=2\n",
    "            res_block = MDCResBlock(current_channels, channels, kernel_sizes, num_kernels_per_scale, stride=1)\n",
    "            max_pool = nn.MaxPool1d(kernel_size=2, stride=2) # Halve the length [cite: 132]\n",
    "            se_block = SEBlock(channels, reduction) # SE block [cite: 133]\n",
    "            dropout = nn.Dropout(dropout_rate) # Dropout [cite: 134]\n",
    "            \n",
    "            block = nn.Sequential(\n",
    "                res_block,\n",
    "                max_pool,\n",
    "                se_block,\n",
    "                dropout\n",
    "            )\n",
    "            self.blocks.append(block)\n",
    "            current_channels = channels # Update channels for next block\n",
    "\n",
    "        self.gap = nn.AdaptiveAvgPool1d(1) # Global Average Pooling [cite: 130]\n",
    "        # The original ConvModel in experiment_10 has FC layers after pooling.\n",
    "        # MDCNet in the paper feeds features to DPNet or directly to Softmax.\n",
    "        # To make it a component usable in the existing structure, we output features before FC layers.\n",
    "        self.output_channels = current_channels\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Input x shape: B x T x C_in (e.g., B x 10 x 115)\n",
    "        # Conv1D expects B x C_in x T\n",
    "        x = x.permute(0, 2, 1) # B x C_in x T (e.g., B x 115 x 10)\n",
    "\n",
    "        for block in self.blocks:\n",
    "            x = block(x)\n",
    "\n",
    "        # Global Average Pooling\n",
    "        x = self.gap(x) # B x C_out x 1\n",
    "        x = x.view(x.size(0), -1) # Flatten: B x C_out\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMMDCModel(nn.Module):\n",
    "    def __init__(self, input_size, time_steps, num_classes=1, \n",
    "                 mdc_block_channels=[64, 64, 64], mdc_kernel_sizes=[3,5,7], \n",
    "                 mdc_kernels_per_scale=2, dropout_rate=0.2, fc_size=32,\n",
    "                 lstm_hidden_size=64, lstm_num_layers=2, lstm_dropout=0.2):\n",
    "        super(LSTMMDCModel, self).__init__()\n",
    "        \n",
    "        # MDCNet branch\n",
    "        self.mdc_net = MDCNet(input_channels=input_size,\n",
    "                              num_classes=num_classes,\n",
    "                              block_channels=mdc_block_channels,\n",
    "                              kernel_sizes=mdc_kernel_sizes,\n",
    "                              num_kernels_per_scale=mdc_kernels_per_scale,\n",
    "                              dropout_rate=dropout_rate)\n",
    "        \n",
    "        # LSTM branch\n",
    "        self.lstm = nn.LSTM(input_size=input_size, \n",
    "                           hidden_size=lstm_hidden_size,\n",
    "                           num_layers=lstm_num_layers, \n",
    "                           batch_first=True,\n",
    "                           dropout=lstm_dropout if lstm_num_layers > 1 else 0,\n",
    "                           bidirectional=True)\n",
    "        \n",
    "        # Get the output feature dimensions\n",
    "        mdc_output_channels = self.mdc_net.output_channels\n",
    "        lstm_output_channels = lstm_hidden_size * 2  # Bidirectional LSTM\n",
    "        combined_features = mdc_output_channels + lstm_output_channels\n",
    "        \n",
    "        # Fully connected layers for classification\n",
    "        self.fc1 = nn.Linear(combined_features, fc_size)\n",
    "        self.relu_fc = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        self.fc2 = nn.Linear(fc_size, num_classes)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Input shape: batch_size x time_steps x features\n",
    "        \n",
    "        # MDC branch processing\n",
    "        mdc_features = self.mdc_net(x)  # Output: B x mdc_output_channels\n",
    "        \n",
    "        # LSTM branch processing\n",
    "        # x is already in format B x T x F, which is what LSTM expects\n",
    "        lstm_out, _ = self.lstm(x)  # Output: B x T x (hidden_size*2)\n",
    "        \n",
    "        # Take the final time step output from LSTM\n",
    "        lstm_features = lstm_out[:, -1, :]  # B x (hidden_size*2)\n",
    "        \n",
    "        # Combine features from both branches\n",
    "        combined = torch.cat([mdc_features, lstm_features], dim=1)\n",
    "        \n",
    "        # Fully connected layers\n",
    "        x = self.fc1(combined)\n",
    "        x = self.relu_fc(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        # Output probability\n",
    "        return self.sigmoid(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the Modified ConvModel\n",
    "input_size = train_X.shape[2]  # Number of features (115)\n",
    "time_steps = train_X.shape[1]  # Number of time steps (10)\n",
    "output_size = 1  # Binary classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model initialized with input_size=86, output_size=1\n"
     ]
    }
   ],
   "source": [
    "# --- MDCNet Hyperparameters ---\n",
    "# Using smaller channels than paper's example for potentially faster training/less memory\n",
    "mdc_block_channels = [16, 32, 64] # Example channel sizes for each block\n",
    "# Using kernel sizes mentioned in paper, and n=2 as found via cross-validation in paper [cite: 242, 243]\n",
    "mdc_kernel_sizes = [3, 5, 7]\n",
    "mdc_kernels_per_scale = 2\n",
    "dropout_rate = 0.2 # Adjusted dropout\n",
    "fc_size = 16 # Adjusted FC size\n",
    "\n",
    "# LSTM parameters\n",
    "lstm_hidden_size = 32\n",
    "lstm_num_layers = 2\n",
    "lstm_dropout = 0.2\n",
    "\n",
    "# Create model instance\n",
    "model = LSTMMDCModel(input_size=input_size,\n",
    "                     time_steps=time_steps,\n",
    "                     num_classes=output_size,\n",
    "                     mdc_block_channels=mdc_block_channels,\n",
    "                     mdc_kernel_sizes=mdc_kernel_sizes,\n",
    "                     mdc_kernels_per_scale=mdc_kernels_per_scale,\n",
    "                     dropout_rate=dropout_rate,\n",
    "                     fc_size=fc_size,\n",
    "                     lstm_hidden_size=lstm_hidden_size,\n",
    "                     lstm_num_layers=lstm_num_layers,\n",
    "                     lstm_dropout=lstm_dropout)\n",
    "\n",
    "print(f\"Model initialized with input_size={input_size}, output_size={output_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "===========================================================================================================================================================\n",
       "Layer (type:depth-idx)                                  Input Shape               Output Shape              Param #                   Mult-Adds\n",
       "===========================================================================================================================================================\n",
       "LSTMMDCModel                                            [10000, 13, 86]           [10000, 1]                --                        --\n",
       "├─MDCNet: 1-1                                           [10000, 13, 86]           [10000, 64]               --                        --\n",
       "│    └─ModuleList: 2-1                                  --                        --                        --                        --\n",
       "│    │    └─Sequential: 3-1                             [10000, 86, 13]           [10000, 16, 6]            11,940                    260,040,000\n",
       "│    │    └─Sequential: 3-2                             [10000, 16, 6]            [10000, 32, 3]            10,400                    51,840,000\n",
       "│    │    └─Sequential: 3-3                             [10000, 32, 3]            [10000, 64, 1]            40,512                    88,960,000\n",
       "│    └─AdaptiveAvgPool1d: 2-2                           [10000, 64, 1]            [10000, 64, 1]            --                        --\n",
       "├─LSTM: 1-2                                             [10000, 13, 86]           [10000, 13, 64]           55,808                    7,255,040,000\n",
       "├─Linear: 1-3                                           [10000, 128]              [10000, 16]               2,064                     20,640,000\n",
       "├─ReLU: 1-4                                             [10000, 16]               [10000, 16]               --                        --\n",
       "├─Dropout: 1-5                                          [10000, 16]               [10000, 16]               --                        --\n",
       "├─Linear: 1-6                                           [10000, 16]               [10000, 1]                17                        170,000\n",
       "├─Sigmoid: 1-7                                          [10000, 1]                [10000, 1]                --                        --\n",
       "===========================================================================================================================================================\n",
       "Total params: 120,741\n",
       "Trainable params: 120,741\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (G): 7.68\n",
       "===========================================================================================================================================================\n",
       "Input size (MB): 44.72\n",
       "Forward/backward pass size (MB): 382.72\n",
       "Params size (MB): 0.26\n",
       "Estimated Total Size (MB): 427.70\n",
       "==========================================================================================================================================================="
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 10000 # Or a smaller representative batch size\n",
    "from torchinfo import summary\n",
    "# try:\n",
    "summary(model, input_size=(batch_size, train_X.shape[1], train_X.shape[2]), device='cpu',\n",
    "        col_names=[\"input_size\", \"output_size\", \"num_params\", \"mult_adds\"], depth=3) # Increased depth to see inside MDCNet\n",
    "# except Exception as e:\n",
    "#     print(f\"Error generating summary, possibly due to dynamic ops or large batch size: {e}\")\n",
    "#     print(\"Trying with smaller batch size for summary...\")\n",
    "#     try:\n",
    "#          summary(model, input_size=(64, train_X.shape[1], train_X.shape[2]), device='cpu',\n",
    "#             col_names=[\"input_size\", \"output_size\", \"num_params\", \"mult_adds\"], depth=3)\n",
    "#     except Exception as e2:\n",
    "#         print(f\"Summary failed even with smaller batch size: {e2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "class TimeSeriesDataset(Dataset):\n",
    "    def __init__(self, data, targets):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            data: numpy array of shape (num_ids, time_steps, features)\n",
    "            targets: numpy array of shape (num_ids,)\n",
    "        \"\"\"\n",
    "        self.data = torch.FloatTensor(data)\n",
    "        self.targets = torch.FloatTensor(targets).unsqueeze(1)  # Add dimension for output\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx], self.targets[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = TimeSeriesDataset(train_X, train_y['target'].values)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataset = TimeSeriesDataset(val_X, val_y['target'].values)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([13, 86]), tensor([0.]))"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset.__getitem__(0)[0].shape, train_dataset.__getitem__(0)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([13, 86]), tensor([1.]))"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_dataset.__getitem__(0)[0].shape, val_dataset.__getitem__(0)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on cuda:0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a7a0f3091b15474e8849f1352f9af74d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1/20 [Train]:   0%|          | 0/29 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ecdabe1474f4fbaa4514fba334162b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1/20 [Valid]:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20 - Train Loss: 0.5073, Val Loss: 0.3204, Val AUC: 0.9292\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a6dec4f7ec0a4fd8b4f33a4c1d2f1b8d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 2/20 [Train]:   0%|          | 0/29 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b23966523765442285b03430bfaa3ac2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 2/20 [Valid]:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/20 - Train Loss: 0.3035, Val Loss: 0.3026, Val AUC: 0.9369\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6135627cd6624e259bfe6d6ea146e7d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 3/20 [Train]:   0%|          | 0/29 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d651ea9cc466499ea03eea42568654f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 3/20 [Valid]:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/20 - Train Loss: 0.2835, Val Loss: 0.2893, Val AUC: 0.9421\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bda5116ff090473e86f0895206bacfd8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 4/20 [Train]:   0%|          | 0/29 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b0fc6cf4fc2a42a79e14c81d4913d5ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 4/20 [Valid]:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/20 - Train Loss: 0.2705, Val Loss: 0.2630, Val AUC: 0.9474\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c858f5dbd99d4101b120531261fc7e06",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 5/20 [Train]:   0%|          | 0/29 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[24], line 45\u001b[0m\n\u001b[0;32m     43\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model(inputs)\n\u001b[0;32m     44\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, labels)\n\u001b[1;32m---> 45\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     46\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m     47\u001b[0m running_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem() \u001b[38;5;241m*\u001b[39m inputs\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\vaseekaranv\\AppData\\Local\\anaconda3\\envs\\nibm_dl\\lib\\site-packages\\torch\\_tensor.py:521\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    511\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    512\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    513\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    514\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    519\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    520\u001b[0m     )\n\u001b[1;32m--> 521\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    522\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    523\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\vaseekaranv\\AppData\\Local\\anaconda3\\envs\\nibm_dl\\lib\\site-packages\\torch\\autograd\\__init__.py:289\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    284\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    286\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    287\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    288\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 289\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    290\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    291\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    292\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    293\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    294\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    295\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    296\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    297\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\vaseekaranv\\AppData\\Local\\anaconda3\\envs\\nibm_dl\\lib\\site-packages\\torch\\autograd\\graph.py:769\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    767\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    768\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 769\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Variable\u001b[38;5;241m.\u001b[39m_execution_engine\u001b[38;5;241m.\u001b[39mrun_backward(  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    770\u001b[0m         t_outputs, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    771\u001b[0m     )  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    772\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    773\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "import time\n",
    "import copy\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import torch.optim as optim\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training parameters\n",
    "num_epochs = 20 # Adjust as needed\n",
    "patience = 3  # Early stopping patience\n",
    "\n",
    "# Initialize variables for early stopping\n",
    "best_val_loss = float('inf')\n",
    "best_val_auc = 0.0\n",
    "best_model_wts = copy.deepcopy(model.state_dict()) # Use state_dict()\n",
    "no_improve_epochs = 0\n",
    "\n",
    "# For tracking metrics\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "val_aucs = []\n",
    "\n",
    "# Move model to device\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "print(f\"Training on {device}\")\n",
    "start_time = time.time()\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    # Training phase\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    train_pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs} [Train]\", leave=False)\n",
    "    for inputs, labels in train_pbar:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "        train_pbar.set_postfix({'loss': loss.item()})\n",
    "\n",
    "    epoch_train_loss = running_loss / len(train_dataset)\n",
    "    train_losses.append(epoch_train_loss)\n",
    "\n",
    "    # Validation phase\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    val_pbar = tqdm(val_loader, desc=f\"Epoch {epoch+1}/{num_epochs} [Valid]\", leave=False)\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in val_pbar:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            all_preds.extend(outputs.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            val_pbar.set_postfix({'loss': loss.item()})\n",
    "\n",
    "    epoch_val_loss = running_loss / len(val_dataset)\n",
    "    val_losses.append(epoch_val_loss)\n",
    "\n",
    "    all_preds = [p[0] for p in all_preds]\n",
    "    all_labels = [l[0] for l in all_labels]\n",
    "    # Handle case where validation set might only have one class temporarily during testing/debugging\n",
    "    if len(np.unique(all_labels)) > 1:\n",
    "        epoch_val_auc = roc_auc_score(all_labels, all_preds)\n",
    "    else:\n",
    "        epoch_val_auc = 0.0 # Or handle as appropriate, e.g., skip AUC calculation\n",
    "    val_aucs.append(epoch_val_auc)\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs} - \"\n",
    "          f\"Train Loss: {epoch_train_loss:.4f}, \"\n",
    "          f\"Val Loss: {epoch_val_loss:.4f}, \"\n",
    "          f\"Val AUC: {epoch_val_auc:.4f}\")\n",
    "\n",
    "    # Check for improvement\n",
    "    if epoch_val_loss < best_val_loss:\n",
    "        best_val_loss = epoch_val_loss\n",
    "        best_val_auc = epoch_val_auc\n",
    "        best_model_wts = copy.deepcopy(model.state_dict())\n",
    "        no_improve_epochs = 0\n",
    "    else:\n",
    "        no_improve_epochs += 1\n",
    "\n",
    "    if no_improve_epochs >= patience:\n",
    "        print(f\"Early stopping triggered after {epoch+1} epochs\")\n",
    "        break\n",
    "\n",
    "# Training complete\n",
    "time_elapsed = time.time() - start_time\n",
    "print(f\"Training completed in {time_elapsed // 60:.0f}m {time_elapsed % 60:.0f}s\")\n",
    "print(f\"Best val loss: {best_val_loss:.4f}, Best val AUC: {best_val_auc:.4f}\")\n",
    "\n",
    "# Load best model weights\n",
    "model.load_state_dict(best_model_wts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to ../../models/deep_learning\\experiment_14.pth\n",
      "Checkpoint saved to ../../models/deep_learning\\experiment_14.pth\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Save the model weights\n",
    "\n",
    "# Create directory if it doesn't exist\n",
    "save_dir = '../../models/deep_learning'\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "# Save model state dictionary\n",
    "model_path = os.path.join(save_dir, f'experiment_{experiment_num}.pth')\n",
    "torch.save(model.state_dict(), model_path)\n",
    "\n",
    "# Save additional information for later reference\n",
    "checkpoint_path = os.path.join(save_dir, f'experiment_{experiment_num}.pth')\n",
    "checkpoint = {\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'optimizer_state_dict': optimizer.state_dict(),\n",
    "}\n",
    "torch.save(checkpoint, checkpoint_path)\n",
    "\n",
    "print(f\"Model saved to {model_path}\")\n",
    "print(f\"Checkpoint saved to {checkpoint_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation predictions obtained.\n",
      "Target Recall: >= 0.9800 for Class 0\n",
      "Threshold found by Binary Search: 0.7047111\n",
      "Achieved Recall at Threshold: 0.9800\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Class 0     0.8503    0.9800    0.9106     23806\n",
      "     Class 1     0.8985    0.5064    0.6477      8318\n",
      "\n",
      "    accuracy                         0.8574     32124\n",
      "   macro avg     0.8744    0.7432    0.7791     32124\n",
      "weighted avg     0.8628    0.8574    0.8425     32124\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0.7047111392021179"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ensure the model is in evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Initialize a list to store predictions\n",
    "val_predictions = []\n",
    "\n",
    "# Disable gradient computation for inference\n",
    "with torch.no_grad():\n",
    "    for inputs, _ in val_loader:\n",
    "        inputs = inputs.to(device)\n",
    "        outputs = model(inputs)\n",
    "        val_predictions.extend(outputs.cpu().numpy())\n",
    "\n",
    "# Flatten the predictions\n",
    "val_predictions = [p[0] for p in val_predictions]\n",
    "print(\"Validation predictions obtained.\")\n",
    "\n",
    "from utils.eval_helpers import evaluate_model_for_recall\n",
    "evaluate_model_for_recall(target_class=0, desired_recall=0.98, y_true=np.array(all_labels).astype('int'), y_pred_proba=np.array(val_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nibm_dl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
